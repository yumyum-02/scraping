# Web Crawler & Scraper

任意のWebサイトをクロールしてスクレイピングし、結果をCSVファイルに出力するNode.jsアプリケーションです。

## 機能

- **サイトクローリング**: 指定したサイトの下層ページを自動で探索
- **タイトル取得**: 各ページのタイトルを取得
- **CSV出力**: 結果をExcelで開けるCSVファイルとして保存
- **エラーハンドリング**: ページ読み込みエラーの適切な処理
- **負荷軽減**: サーバーに負荷をかけないよう適切な遅延設定

## 必要環境

- Node.js (v14以上推奨)
- npm

## インストール

1. リポジトリをクローンまたはダウンロード
2. 依存関係をインストール

```bash
npm install
```

## 使用方法

### 基本的な使用方法

```bash
node crowl.js <サイトURL>
```

### 使用例

```bash
# 基本的なクローリング
node crowl.js https://example.com

# 実際のサイトでの使用例
node crowl.js https://github.com
node crowl.js https://google.com
```

## 出力

実行すると以下の情報が取得されます：

1. **メインページのタイトル**
2. **見つかったリンク数**
3. **各下層ページのタイトル**

結果は `crawl_results.csv` ファイルに保存され、以下の形式で出力されます：

```csv
URL,タイトル,ページタイプ
"https://example.com","サイトタイトル","メインページ"
"https://example.com/about","会社概要","下層ページ"
"https://example.com/services","サービス","下層ページ"
```

## Excelでの確認方法

1. **直接開く**: Excelを起動して「ファイルを開く」で `crawl_results.csv` を選択
2. **ドラッグ&ドロップ**: CSVファイルをExcelにドラッグ&ドロップ
3. **データタブ**: Excelの「データ」タブから「テキストまたはCSVから」を選択

## 技術仕様

- **Puppeteer**: ブラウザ自動化ライブラリ
- **Node.js**: JavaScript実行環境
- **CSV出力**: Excel互換のCSV形式

## 注意事項

- クローリングする際は、対象サイトの利用規約を確認してください
- サーバーに過度な負荷をかけないよう、適切な間隔でリクエストを送信しています
- 大量のページをクロールする場合は、サイトの負荷を考慮してください

## トラブルシューティング

### よくある問題

1. **Puppeteerがインストールされていない**
   ```bash
   npm install puppeteer
   ```

2. **URLが指定されていない**
   ```bash
   node crowl.js https://example.com
   ```

3. **ページの読み込みに時間がかかる**
   - ネットワーク環境を確認
   - 対象サイトの応答速度を確認